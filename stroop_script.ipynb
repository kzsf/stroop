{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "import yaml\n",
    "\n",
    "#read the config file and get input/output paths\n",
    "with open('paths.yml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "input_path = config['your_paths']['input_path']\n",
    "output_path = config['your_paths']['output_path']\n",
    "\n",
    "#read in excel spreadsheet\n",
    "sheets = pd.read_excel(input_path, sheet_name=None, header=None)\n",
    "\n",
    "#define the values we want in column E (index 4)\n",
    "valid_values = ['conM', 'conMNM', 'conNM', 'incM', 'incMNM', 'incNM']\n",
    "\n",
    "filtered_sheets = {}\n",
    "\n",
    "#loop over each sheet and filter data based on valid values in column E\n",
    "for sheet_name, sheet_data in sheets.items():\n",
    "    #filter the sheet based on the values in column E (index 4)\n",
    "    filtered_data = sheet_data[sheet_data.iloc[:, 4].isin(valid_values)]\n",
    "    filtered_data = filtered_data.iloc[:, :8]\n",
    "    filtered_data.columns = ['Trial', 'RunLabel', 'Condition', 'TrialStart', 'EventTag', 'Time', 'keys', 'match_status']\n",
    "    filtered_sheets[sheet_name] = filtered_data\n",
    "\n",
    "for sheet_name, sheet_data in filtered_sheets.items():\n",
    "    #drop duplicates to get rid of extra colnames\n",
    "    sheet_data = sheet_data.drop_duplicates()\n",
    "    #update the filtered_sheets with the cleaned data\n",
    "    filtered_sheets[sheet_name] = sheet_data\n",
    "\n",
    "#define the values we want in column E\n",
    "valid_values = ['conM', 'conMNM', 'conNM', 'incM', 'incMNM', 'incNM', 'EventTag']\n",
    "\n",
    "#define the colnames\n",
    "valid_row_values = ['Trial', 'RunLabel', 'Condition', 'trial start', 'EventTag', 'Time', 'keys', 'sequence', 'mouse_down']\n",
    "\n",
    "filtered_sheets_2 = {}\n",
    "\n",
    "#filter each sheet based on valid values in column E (index 4)\n",
    "for sheet_name, sheet_data in sheets.items():\n",
    "    filtered_data = sheet_data[sheet_data.iloc[:, 4].isin(valid_values)]\n",
    "    filtered_data = filtered_data.reset_index(drop=True)\n",
    "    filtered_sheets_2[sheet_name] = filtered_data\n",
    "\n",
    "#set colnames, subset first 15 rows and delete dupes\n",
    "for sheet_name, sheet_data in filtered_sheets_2.items():\n",
    "    sheet_data.columns = sheet_data.iloc[0]\n",
    "    sheet_data = sheet_data.drop(index=0).reset_index(drop=True)\n",
    "    \n",
    "    first_15_rows = sheet_data.head(15)\n",
    "    first_15_rows = first_15_rows.drop_duplicates(keep='first')\n",
    "\n",
    "    sheet_data = pd.concat([first_15_rows, sheet_data.iloc[15:]]).reset_index(drop=True)\n",
    "    filtered_sheets_2[sheet_name] = sheet_data\n",
    "\n",
    "#function that splits the data into separate blocks (individual dfs) for each run (based on each occurrence of header row)\n",
    "def split_dataframe_by_header(sheet_data):\n",
    "    blocks = []\n",
    "    header_indices = sheet_data[sheet_data.iloc[:, 0] == valid_row_values[0]].index.tolist()\n",
    "    header_indices.append(len(sheet_data))\n",
    "\n",
    "    #split the df into blocks\n",
    "    for i in range(len(header_indices) - 1):\n",
    "        start = header_indices[i]\n",
    "        end = header_indices[i + 1]\n",
    "        block = sheet_data.iloc[start:end].reset_index(drop=True)\n",
    "        \n",
    "        #if first row of the block is a header, drop it (already used as colname)\n",
    "        if block.iloc[0].tolist() == valid_row_values:\n",
    "            block = block.drop(index=0).reset_index(drop=True)\n",
    "\n",
    "        blocks.append(block)\n",
    "\n",
    "    return blocks\n",
    "\n",
    "#test_subj = filtered_sheets_2['HP23-01696']\n",
    "\n",
    "#function to remove any extra characters in 'keys' column (such as _5UP)\n",
    "def clean_keys_column(sheet_data):\n",
    "    sheet_data['keys'] = sheet_data['keys'].str.replace(r'\\[([0-9]+)\\].*', r'[\\1]', regex=True)\n",
    "    return sheet_data\n",
    "\n",
    "#function to find the most common value in 'keys' for specific EventTag section\n",
    "def most_common_key_in_section(block, event_tag):\n",
    "    section_keys = block[block['EventTag'] == event_tag]['keys']\n",
    "    mode_result = section_keys.mode()\n",
    "    return mode_result[0] if not mode_result.empty else None\n",
    "\n",
    "\n",
    "def map_condition(condition_value, match_key='[1]', nonmatch_key='[2]'):\n",
    "    \"\"\"\n",
    "    Maps conditions to expected responses based on participant's response pattern.\n",
    "    \"\"\"\n",
    "    if pd.isna(condition_value):\n",
    "        return condition_value\n",
    "    \n",
    "    if 'conM' in condition_value or 'incM' in condition_value:\n",
    "        return match_key\n",
    "    elif 'conNM' in condition_value or 'incNM' in condition_value:\n",
    "        return nonmatch_key\n",
    "    else:\n",
    "        return condition_value\n",
    "    \n",
    "\n",
    "#function to replace most common keys in the block\n",
    "def replace_most_common_keys(block, conM_key, conNM_key):\n",
    "    if conM_key:\n",
    "        block.loc[:, 'keys'] = block['keys'].replace(conM_key, '[1]')\n",
    "    if conNM_key:\n",
    "        block.loc[:, 'keys'] = block['keys'].replace(conNM_key, '[2]')\n",
    "    return block\n",
    "\n",
    "#function to drop extra columns if more than 9\n",
    "def drop_extra_columns(sheet_data):\n",
    "    if sheet_data.shape[1] > 9:\n",
    "        sheet_data = sheet_data.drop(columns=sheet_data.columns[9])\n",
    "        #print(f\"Extra column dropped.\")\n",
    "    return sheet_data\n",
    "\n",
    "def process_block_data(block):\n",
    "    block = clean_block_data(block)\n",
    "    \n",
    "    #convert time to numeric\n",
    "    block['Time'] = pd.to_numeric(block['Time'], errors='coerce')\n",
    "    \n",
    "    #remove missing values\n",
    "    block = block[block['keys'].notna()]\n",
    "    \n",
    "    #determine participant's response mapping\n",
    "    match_key, nonmatch_key = determine_response_mapping(block)\n",
    "    \n",
    "    if not match_key or not nonmatch_key:\n",
    "        match_key = '[1]'\n",
    "        nonmatch_key = '[2]'\n",
    "    \n",
    "    block = block.copy()\n",
    "    #use the determined mapping to set conditions\n",
    "    block['Condition'] = block['Condition'].apply(\n",
    "        lambda x: map_condition(x, match_key=match_key, nonmatch_key=nonmatch_key)\n",
    "    )\n",
    "    \n",
    "    #count errors with correct mapping\n",
    "    error_mask = block['Condition'] != block['keys']\n",
    "    error_count = error_mask.sum()\n",
    "    \n",
    "    #subset errors\n",
    "    error_block = block[error_mask]\n",
    "    \n",
    "    #remove errors\n",
    "    block = block[~error_mask]\n",
    "    \n",
    "    return block, error_count, error_block\n",
    "\n",
    "#function to clean the block data (drop last two columns, drop rows that match header row, etc.)\n",
    "def clean_block_data(block):\n",
    "    block = block.iloc[:, :-2]  \n",
    "    if block.iloc[-1].tolist() == valid_row_values:\n",
    "        block = block.drop(block.index[-1])  \n",
    "    return block\n",
    "\n",
    "#function to remove any extra characters in 'keys' column (such as _5UP) in block\n",
    "def clean_keys_column_block(block):\n",
    "    block['keys'] = block['keys'].str.replace(r'\\[([0-9]+)\\].*', r'[\\1]', regex=True)\n",
    "    return block\n",
    "\n",
    "def update_subject_metrics(block, subject_metrics):\n",
    "    #get error count and cleaned block\n",
    "    block_cleaned, errors_in_block, error_block = process_block_data(block)\n",
    "    subject_metrics['total_errors'] += errors_in_block\n",
    "\n",
    "    #use cleaned block for remaining calculations\n",
    "    real_missed_trials, fake_missed_trials, duplicated_trials = count_trials(block_cleaned)\n",
    "    subject_metrics['real_missed_trials'] += real_missed_trials\n",
    "    subject_metrics['fake_missed_trials'] += fake_missed_trials\n",
    "    subject_metrics['duplicated_trials'] += duplicated_trials\n",
    "\n",
    "    subject_metrics['total_time'] += block_cleaned['Time'].sum()\n",
    "    subject_metrics['total_rows'] += len(block_cleaned)\n",
    "    \n",
    "    return block_cleaned\n",
    "\n",
    "def determine_response_mapping(block):\n",
    "    #get responses for match and nonmatch conditions\n",
    "    match_responses = block[block['EventTag'].str.contains('M', na=False) & \n",
    "                          ~block['EventTag'].str.contains('NM', na=False)]['keys']\n",
    "    nonmatch_responses = block[block['EventTag'].str.contains('NM', na=False)]['keys']\n",
    "    \n",
    "    #find most common response for each condition\n",
    "    match_key = match_responses.mode().iloc[0] if not match_responses.empty else None\n",
    "    nonmatch_key = nonmatch_responses.mode().iloc[0] if not nonmatch_responses.empty else None\n",
    "    \n",
    "    return match_key, nonmatch_key\n",
    "\n",
    "#function to count real missed trials, fake missed trials, and duplicated trials\n",
    "def count_trials(block):\n",
    "    trial_numbers = block['Trial'].astype(int)\n",
    "    event_tags = block['EventTag']\n",
    "\n",
    "    real_missed_trials = 0\n",
    "    fake_missed_trials = 0\n",
    "    duplicated_trials = 0\n",
    "\n",
    "    for i in range(1, len(trial_numbers)):\n",
    "        prev_trial = trial_numbers.iloc[i - 1]\n",
    "        curr_trial = trial_numbers.iloc[i]\n",
    "        prev_event = event_tags.iloc[i - 1]\n",
    "        curr_event = event_tags.iloc[i]\n",
    "\n",
    "        if curr_trial > prev_trial + 1 and prev_event == curr_event:\n",
    "            real_missed_trials += 1\n",
    "        elif curr_trial > prev_trial + 1 and prev_event != curr_event:\n",
    "            fake_missed_trials += 1\n",
    "\n",
    "    duplicated_trials = trial_numbers[trial_numbers.duplicated()].nunique()\n",
    "    return real_missed_trials, fake_missed_trials, duplicated_trials\n",
    "\n",
    "\n",
    "#function to calculate the average time for the subject\n",
    "def calculate_average_time(total_time, total_rows):\n",
    "    return total_time / total_rows if total_rows else 0\n",
    "\n",
    "#function to initialize the subject's metrics\n",
    "def initialize_subject_metrics():\n",
    "    return {\n",
    "        'real_missed_trials': 0,\n",
    "        'fake_missed_trials': 0,\n",
    "        'duplicated_trials': 0,\n",
    "        'total_errors': 0,\n",
    "        'total_time': 0,\n",
    "        'total_rows': 0,\n",
    "        'average_time': 0\n",
    "    }\n",
    "\n",
    "def initialize_subject_metrics_2():\n",
    "    return{\n",
    "        'std': 0,\n",
    "        '4std': 0,\n",
    "        'outlier_cutoff': 0\n",
    "    }\n",
    "     \n",
    "def process_subject_data(filtered_sheets_2):\n",
    "    subject_trial_counts = {}\n",
    "    processed_sheets = {}\n",
    "    cleaned_sheets = {}\n",
    "    error_sheets = {}\n",
    "\n",
    "    for sheet_name, sheet_data in filtered_sheets_2.items():\n",
    "        subject_metrics = initialize_subject_metrics()\n",
    "        sheet_data = drop_extra_columns(sheet_data)\n",
    "        sheet_data = clean_keys_column(sheet_data)\n",
    "        blocks = split_dataframe_by_header(sheet_data)\n",
    "        \n",
    "        processed_blocks = []\n",
    "        error_blocks = []\n",
    "        \n",
    "        for block in blocks:\n",
    "            if not block.empty:\n",
    "                block = clean_keys_column_block(block)\n",
    "                #process the block and update metrics\n",
    "                cleaned_block, errors_in_block, error_block = process_block_data(block)\n",
    "                \n",
    "                #update metrics\n",
    "                subject_metrics['total_errors'] += errors_in_block\n",
    "                \n",
    "                #count trials for this block\n",
    "                real_missed, fake_missed, duplicated = count_trials(cleaned_block)\n",
    "                subject_metrics['real_missed_trials'] += real_missed\n",
    "                subject_metrics['fake_missed_trials'] += fake_missed\n",
    "                subject_metrics['duplicated_trials'] += duplicated\n",
    "                \n",
    "                #update time and row counts\n",
    "                subject_metrics['total_time'] += cleaned_block['Time'].sum()\n",
    "                subject_metrics['total_rows'] += len(cleaned_block)\n",
    "                \n",
    "                processed_blocks.append(cleaned_block)\n",
    "                if not error_block.empty:\n",
    "                    error_blocks.append(error_block)\n",
    "\n",
    "        #combine all cleaned blocks\n",
    "        if processed_blocks:\n",
    "            processed_sheet = pd.concat(processed_blocks, ignore_index=True)\n",
    "            processed_sheets[sheet_name] = processed_sheet\n",
    "            \n",
    "        #combine all error blocks for this subject\n",
    "        if error_blocks:\n",
    "            error_sheet = pd.concat(error_blocks, ignore_index=True)\n",
    "            error_sheets[sheet_name] = error_sheet\n",
    "\n",
    "        subject_metrics['average_time'] = calculate_average_time(\n",
    "            subject_metrics['total_time'], \n",
    "            subject_metrics['total_rows']\n",
    "        )\n",
    "        \n",
    "        #store the metrics for this subject\n",
    "        subject_trial_counts[sheet_name] = subject_metrics\n",
    "\n",
    "        #remove header rows\n",
    "        if processed_sheets.get(sheet_name) is not None:\n",
    "            header_mask = processed_sheets[sheet_name]['Trial'] == 'Trial'\n",
    "            sheet_data = processed_sheets[sheet_name][~header_mask].copy()\n",
    "\n",
    "            subject_metrics['std'] = sheet_data['Time'].std()\n",
    "            subject_metrics['4std'] = 4 * subject_metrics['std']\n",
    "            subject_metrics['outlier_cutoff'] = subject_metrics['4std'] + subject_metrics['average_time']\n",
    "\n",
    "            #remove outliers\n",
    "            outlier_mask = processed_sheets[sheet_name]['Time'] >= subject_metrics['outlier_cutoff']\n",
    "            subject_metrics['total_outliers'] = outlier_mask.sum()\n",
    "            cleaned_sheet = processed_sheets[sheet_name][~outlier_mask].copy()\n",
    "            cleaned_sheets[sheet_name] = cleaned_sheet\n",
    "\n",
    "            subject_metrics['mean'] = cleaned_sheet['Time'].mean()\n",
    "            subject_metrics['median'] = cleaned_sheet['Time'].median()\n",
    "            subject_metrics['stdev'] = cleaned_sheet['Time'].std()\n",
    "\n",
    "    return subject_trial_counts, filtered_sheets_2, cleaned_sheets, error_sheets\n",
    "                  \n",
    "#call the main function\n",
    "subject_trial_counts, filtered_sheets_2, cleaned_sheets, error_sheets = process_subject_data(filtered_sheets_2)\n",
    "\n",
    "def analyze_errors(error_sheets):\n",
    "    for subject, error_data in error_sheets.items():\n",
    "        print(f\"\\nErrors for subject {subject}:\")\n",
    "        print(f\"Total error trials: {len(error_data)}\")\n",
    "        \n",
    "        print(\"\\nError distribution by condition:\")\n",
    "        print(error_data['EventTag'].value_counts())\n",
    "        \n",
    "        print(\"\\nSample of error trials:\")\n",
    "        print(error_data[['EventTag', 'Condition', 'keys', 'Time']].head())\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "subject_trial_counts, filtered_sheets_2, cleaned_sheets, error_sheets = process_subject_data(filtered_sheets_2)\n",
    "\n",
    "conditional_df = {}\n",
    "conditional_stats = {}\n",
    "\n",
    "for sheet_name, sheet_data in cleaned_sheets.items(): \n",
    "    conditional_df[sheet_name] = sheet_data.sort_values(by = 'EventTag', ascending = True)\n",
    "    current_sorted = conditional_df[sheet_name]\n",
    "\n",
    "    #conM-RR \n",
    "    conM_df = current_sorted[current_sorted['EventTag'] == 'conM']\n",
    "\n",
    "    #incM-RR\n",
    "    incM_df = current_sorted[current_sorted['EventTag'] == 'incM']\n",
    "\n",
    "    #conM-RS \n",
    "    #df that is all rows other than the conM rr ones\n",
    "    non_conM_df = current_sorted[current_sorted['EventTag'] != 'conM']\n",
    "    #df that is all conM Conditions that are in non_conM_df aka all the rs conM's\n",
    "    con_rs_df = non_conM_df[non_conM_df['EventTag'].str.contains('conM', case=False, na=False)]\n",
    "    conM_rs_df = con_rs_df[con_rs_df['Condition'] == '[1]']\n",
    "\n",
    "    #incM-RS\n",
    "    rs_df = current_sorted[current_sorted['EventTag'].str.contains('MNM', case=False, na=False)]\n",
    "    inc_rs_df = rs_df[rs_df['EventTag'].str.contains('inc', case=False, na=False)]\n",
    "    incM_rs_df = inc_rs_df[inc_rs_df['Condition'] == '[1]']\n",
    "\n",
    "    #conNM-RR\n",
    "    conNM_df = current_sorted[current_sorted['EventTag'] == 'conNM']\n",
    "\n",
    "    #incNM-RR\n",
    "    incNM_df = current_sorted[current_sorted['EventTag'] == 'incNM']\n",
    "\n",
    "    #conNM-RS\n",
    "    #df that is all rows other than the conNM rr ones\n",
    "    non_conNM_df = current_sorted[current_sorted['EventTag'] != 'conNM']\n",
    "    #df that is all conNM Conditions that are in non_conNM_df aka all the rs conNM's\n",
    "    #conNM_rs_df = non_conNM_df[non_conNM_df['EventTag'].str.contains('conNM', case=False, na=False)]\n",
    "    conNM_rs_df = non_conNM_df[non_conNM_df['EventTag'] == 'conMNM']\n",
    "    conNM_rs_df = conNM_rs_df[conNM_rs_df['Condition'] == '[2]']\n",
    "\n",
    "    #incNM-RS\n",
    "    #df that is all rows other than the incNM rr ones\n",
    "    non_incNM_df = current_sorted[current_sorted['EventTag'] != 'incNM']\n",
    "    #df that is all incNM Conditions that are in non_incNM_df aka all the rs incNM's\n",
    "    incNM_rs_df = non_incNM_df[non_incNM_df['EventTag'] == 'incMNM']\n",
    "    incNM_rs_df = incNM_rs_df[incNM_rs_df['Condition'] == '[2]']\n",
    "\n",
    "    #con-RR\n",
    "    #remove response switching\n",
    "    rr_df = current_sorted[~current_sorted['EventTag'].str.contains('MNM', case=False, na=False)]\n",
    "    con_rr_df = rr_df[rr_df['EventTag'].str.contains('con', case=False, na=False)]\n",
    "\n",
    "    #inc-RR\n",
    "    inc_rr_df = rr_df[rr_df['EventTag'].str.contains('inc', case=False, na=False)]\n",
    "\n",
    "    #con-RS \n",
    "    #df that is all rows other than the conM rr ones\n",
    "    non_conM_df = current_sorted[current_sorted['EventTag'] != 'conM']\n",
    "    #df that is all conM Conditions that are in non_conM_df aka all the rs conM's\n",
    "    con_rs_df = non_conM_df[non_conM_df['EventTag'].str.contains('conM', case=False, na=False)]\n",
    "\n",
    "    #inc-RS\n",
    "    #subset response switching\n",
    "    rs_df = current_sorted[current_sorted['EventTag'].str.contains('MNM', case=False, na=False)]\n",
    "    inc_rs_df = rs_df[rs_df['EventTag'].str.contains('inc', case=False, na=False)]\n",
    "\n",
    "    #update conditional stats with this sheet's metrics\n",
    "    conditional_stats[sheet_name] = {\n",
    "\n",
    "        'conM-RR mean': conM_df['Time'].mean(),\n",
    "        'conM-RR median': conM_df['Time'].median(),\n",
    "        'conM-RR stdev': conM_df['Time'].std(),\n",
    "\n",
    "        'incM-RR mean': incM_df['Time'].mean(),\n",
    "        'incM-RR median': incM_df['Time'].median(),\n",
    "        'incM-RR stdev': incM_df['Time'].std(),\n",
    "\n",
    "        'conM-RS mean': conM_rs_df['Time'].mean(),\n",
    "        'conM-RS median': conM_rs_df['Time'].median(),\n",
    "        'conM-RS stdev': conM_rs_df['Time'].std(),\n",
    "\n",
    "        'incM-RS mean': incM_rs_df['Time'].mean(),\n",
    "        'incM-RS median': incM_rs_df['Time'].median(),\n",
    "        'incM-RS stdev': incM_rs_df['Time'].std(),\n",
    "\n",
    "        'conNM-RR mean': conNM_df['Time'].mean(),\n",
    "        'conNM-RR median': conNM_df['Time'].median(),\n",
    "        'conNM-RR stdev': conNM_df['Time'].std(),\n",
    "\n",
    "        'incNM-RR mean': incNM_df['Time'].mean(),\n",
    "        'incNM-RR median': incNM_df['Time'].median(),\n",
    "        'incNM-RR stdev': incNM_df['Time'].std(),\n",
    "\n",
    "        'conNM-RS mean': conNM_rs_df['Time'].mean(),\n",
    "        'conNM-RS median': conNM_rs_df['Time'].median(),\n",
    "        'conNM-RS stdev': conNM_rs_df['Time'].std(),\n",
    "\n",
    "        'incNM-RS mean': incNM_rs_df['Time'].mean(),\n",
    "        'incNM-RS median': incNM_rs_df['Time'].median(),\n",
    "        'incNM-RS stdev': incNM_rs_df['Time'].std(),\n",
    "\n",
    "        'con-RR mean': con_rr_df['Time'].mean(),\n",
    "        'con-RR median': con_rr_df['Time'].median(),\n",
    "        'con-RR stdev': con_rr_df['Time'].std(),\n",
    "\n",
    "        'inc-RR mean': inc_rr_df['Time'].mean(),\n",
    "        'inc-RR median': inc_rr_df['Time'].median(),\n",
    "        'inc-RR stdev': inc_rr_df['Time'].std(),\n",
    "\n",
    "        'con-RS mean': con_rs_df['Time'].mean(),\n",
    "        'con-RS median': con_rs_df['Time'].median(),\n",
    "        'con-RS stdev': con_rs_df['Time'].std(),\n",
    "\n",
    "        'inc-RS mean': inc_rs_df['Time'].mean(),\n",
    "        'inc-RS median': inc_rs_df['Time'].median(),\n",
    "        'inc-RS stdev': inc_rs_df['Time'].std(),\n",
    "    }\n",
    "\n",
    "'''\n",
    "print tallied results for each subject\n",
    "for subject, counts in subject_trial_counts.items():\n",
    "    print(f\"\\nTally for {subject}:\")\n",
    "    print(f\"  - Real missed trials: {counts['real_missed_trials']}\")\n",
    "    print(f\"  - Fake missed trials (block switches): {counts['fake_missed_trials']}\")\n",
    "    print(f\"  - Duplicate trials: {counts['duplicated_trials']}\")\n",
    "    print(f\"  - Errors: {counts['total_errors']}\")\n",
    "    print(f\"  - Mean Time Errors Removed: {counts['average_time']:.3f}\")\n",
    "    print(f\"  - Standard Deviation Errors Removed: {counts['std']:.3f}\")\n",
    "    print(f\"  - Outlier Cutoff: {counts['outlier_cutoff']:.3f}\")\n",
    "    print(f\"  - Number outliers (prolonged): {counts['total_outliers']}\")\n",
    "    print(f\"  - Mean Time Outliers and Errors Removed: {counts['mean']:.2f}\")\n",
    "    print(f\"  - Median Outliers and Errors Removed: {counts['median']:.2f}\")\n",
    "    print(f\"  - SD Time Outliers and Errors Removed: {counts['stdev']:.2f}\")\n",
    "\n",
    "#export conditional stats to csv with subject_id\n",
    "conditional_stats = pd.DataFrame.from_dict(conditional_stats).T\n",
    "conditional_stats.to_csv('/Users/kjung6/Eva/Stroop/final_dataset/2-11-25_conditional_stats.csv', index_label='Subject_ID')\n",
    "\n",
    "#export subject trial counts as csv\n",
    "subject_trial_counts = pd.DataFrame.from_dict(subject_trial_counts)\n",
    "subject_trial_counts.to_csv('/Users/kjung6/Eva/Stroop/final_dataset/2-11-25_subject_trial_counts.csv', index_label='Statistics')\n",
    "\n",
    "#export subject trial counts as csv TRANSPOSED\n",
    "subject_trial_counts = pd.DataFrame.from_dict(subject_trial_counts).T\n",
    "subject_trial_counts.to_csv('/Users/kjung6/Eva/Stroop/final_dataset/2-11-25_subject_trial_counts_T.csv', index_label='Subject_ID')\n",
    "'''\n",
    "\n",
    "#convert to pd df, make Subject_ID the first column, then combine\n",
    "subject_trial_counts = pd.DataFrame.from_dict(subject_trial_counts).T\n",
    "subject_trial_counts = subject_trial_counts.reset_index()\n",
    "subject_trial_counts = subject_trial_counts.rename(columns = {'index' : 'Subject_ID'})\n",
    "\n",
    "conditional_stats = pd.DataFrame.from_dict(conditional_stats).T\n",
    "conditional_stats = conditional_stats.reset_index()\n",
    "conditional_stats = conditional_stats.rename(columns = {'index' : 'Subject_ID'})\n",
    "combined_stats = pd.merge(subject_trial_counts, conditional_stats, on = \"Subject_ID\", how = \"left\")\n",
    "\n",
    "#export final combined stats\n",
    "combined_stats.to_csv(f'{output_path}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
